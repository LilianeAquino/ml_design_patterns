{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6193b3",
   "metadata": {},
   "source": [
    "# Design Pattern 12: Checkpoints\n",
    "\n",
    "> Padrão usado em treinamento de modelos para salvar periodicamente o estado completo do modelo durante o processo de treinamento. Isso é feito para que modelos parcialmente treinados estejam disponíveis e possam ser usados como modelos finais (no caso de parada antecipada) ou como pontos de partida para treinamentos subsequentes (como em caso de falha da máquina ou ajuste fino)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25575721",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5631637",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-24 17:47:38.035093: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-24 17:47:38.228822: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-24 17:47:39.622299: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-24 17:47:39.627954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-24 17:47:41.141391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b1e5e",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f9755",
   "metadata": {},
   "source": [
    "#### Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0a47c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 6s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c27ad1",
   "metadata": {},
   "source": [
    "#### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "883c84f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d8e58",
   "metadata": {},
   "source": [
    "#### Checkpoint: Steps per epoch\n",
    "\n",
    "> Em vez de treinar o modelo por um número fixo de épocas podemos treiná-lo por um número específico de passos onde cada passo corresponde a uma atualização dos pesos do modelo com um lote de dados. Isso nos permite ter mais granularidade no controle do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a07bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "9533/9533 [==============================] - 17s 2ms/step - loss: 0.0650 - accuracy: 0.9819 - val_loss: 0.0818 - val_accuracy: 0.9778\n",
      "Epoch 2/15\n",
      "9533/9533 [==============================] - 17s 2ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0941 - val_accuracy: 0.9809\n",
      "Epoch 3/15\n",
      "9533/9533 [==============================] - 17s 2ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.1060 - val_accuracy: 0.9809\n",
      "Epoch 4/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.1342 - val_accuracy: 0.9794\n",
      "Epoch 5/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.1250 - val_accuracy: 0.9807\n",
      "Epoch 6/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 7.9010e-04 - accuracy: 0.9998 - val_loss: 0.1390 - val_accuracy: 0.9800\n",
      "Epoch 7/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 8.3202e-04 - accuracy: 0.9998 - val_loss: 0.1449 - val_accuracy: 0.9797\n",
      "Epoch 8/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 5.6191e-04 - accuracy: 0.9998 - val_loss: 0.2399 - val_accuracy: 0.9697\n",
      "Epoch 9/15\n",
      "9533/9533 [==============================] - 17s 2ms/step - loss: 4.0837e-04 - accuracy: 0.9999 - val_loss: 0.1577 - val_accuracy: 0.9811\n",
      "Epoch 10/15\n",
      "9533/9533 [==============================] - 18s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.1589 - val_accuracy: 0.9814\n",
      "Epoch 11/15\n",
      "9533/9533 [==============================] - 17s 2ms/step - loss: 6.5798e-04 - accuracy: 0.9998 - val_loss: 0.1833 - val_accuracy: 0.9792\n",
      "Epoch 12/15\n",
      "9533/9533 [==============================] - 19s 2ms/step - loss: 6.3235e-05 - accuracy: 1.0000 - val_loss: 0.1721 - val_accuracy: 0.9815\n",
      "Epoch 13/15\n",
      "9533/9533 [==============================] - 18s 2ms/step - loss: 8.1985e-04 - accuracy: 0.9998 - val_loss: 0.1841 - val_accuracy: 0.9811\n",
      "Epoch 14/15\n",
      "9533/9533 [==============================] - 17s 2ms/step - loss: 1.9522e-06 - accuracy: 1.0000 - val_loss: 0.1850 - val_accuracy: 0.9810\n",
      "Epoch 15/15\n",
      "9533/9533 [==============================] - 17s 2ms/step - loss: 1.4136e-07 - accuracy: 1.0000 - val_loss: 0.1973 - val_accuracy: 0.9807\n",
      "100/100 - 0s - loss: 0.1973 - accuracy: 0.9807 - 88ms/epoch - 877us/step\n",
      "\n",
      "Test accuracy: 0.9807000160217285\n"
     ]
    }
   ],
   "source": [
    "# Número total de passos de treinamento que serão realizados.\n",
    "NUM_STEPS = 143000\n",
    "# Tamanho de cada lote de dados usado durante o treinamento.\n",
    "BATCH_SIZE = 100\n",
    "# Quantos checkpoints serão salvos durante o treinamento.\n",
    "NUM_CHECKPOINTS = 15\n",
    "\n",
    "# Define o caminho onde os pesos do modelo serão salvos.\n",
    "checkpoint_path = './checkpoints/model_checkpoint'\n",
    "# Callback ModelCheckpoint que salva os pesos do modelo em checkpoint_path\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_freq=NUM_STEPS // NUM_CHECKPOINTS)\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "\n",
    "# Embaralha, agrupa os dados em lotes e repete o dataset indefinidamente. \n",
    "# A repetição é necessária para garantir que o modelo veja os dados continuamente durante o treinamento.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "eval_dataset = eval_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=NUM_CHECKPOINTS, # Cada época virtual é definida pelo número de checkpoints\n",
    "                    steps_per_epoch=NUM_STEPS // NUM_CHECKPOINTS, # Número de passos por época\n",
    "                    validation_data=eval_dataset,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(eval_dataset, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe5f14",
   "metadata": {},
   "source": [
    "#### Checkpoint: Virtual epochs\n",
    "\n",
    "> O conceito de \"Virtual epochs\" visa manter constante o número total de exemplos de treinamento que o modelo vê, independentemente de quantos dados são adicionados ao longo do tempo. Isso é feito ajustando o número de passos por época com base no número total desejado de exemplos de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e35dfd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 1.7677e-08 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9807\n",
      "Epoch 2/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 7.7600e-09 - accuracy: 1.0000 - val_loss: 0.2100 - val_accuracy: 0.9808\n",
      "Epoch 3/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 5.0331e-09 - accuracy: 1.0000 - val_loss: 0.2129 - val_accuracy: 0.9809\n",
      "Epoch 4/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 3.8166e-09 - accuracy: 1.0000 - val_loss: 0.2150 - val_accuracy: 0.9809\n",
      "Epoch 5/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 3.1356e-09 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.9806\n",
      "Epoch 6/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.7284e-09 - accuracy: 1.0000 - val_loss: 0.2187 - val_accuracy: 0.9807\n",
      "Epoch 7/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.4934e-09 - accuracy: 1.0000 - val_loss: 0.2203 - val_accuracy: 0.9807\n",
      "Epoch 8/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.3562e-09 - accuracy: 1.0000 - val_loss: 0.2219 - val_accuracy: 0.9805\n",
      "Epoch 9/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.2977e-09 - accuracy: 1.0000 - val_loss: 0.2231 - val_accuracy: 0.9807\n",
      "Epoch 10/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.2949e-09 - accuracy: 1.0000 - val_loss: 0.2246 - val_accuracy: 0.9809\n",
      "Epoch 11/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.2834e-09 - accuracy: 1.0000 - val_loss: 0.2259 - val_accuracy: 0.9807\n",
      "Epoch 12/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.3029e-09 - accuracy: 1.0000 - val_loss: 0.2273 - val_accuracy: 0.9807\n",
      "Epoch 13/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.3305e-09 - accuracy: 1.0000 - val_loss: 0.2292 - val_accuracy: 0.9806\n",
      "Epoch 14/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.3797e-09 - accuracy: 1.0000 - val_loss: 0.2303 - val_accuracy: 0.9809\n",
      "Epoch 15/15\n",
      "9533/9533 [==============================] - 16s 2ms/step - loss: 2.4207e-09 - accuracy: 1.0000 - val_loss: 0.2319 - val_accuracy: 0.9805\n",
      "100/100 - 0s - loss: 0.2319 - accuracy: 0.9805 - 112ms/epoch - 1ms/step\n",
      "\n",
      "Test accuracy: 0.9804999828338623\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAINING_EXAMPLES = 1000000 # Número total de exemplos de treinamento inicial\n",
    "STOP_POINT = 14.3  # Ponto de parada virtual que indica quantas \"épocas virtuais\" são desejadas\n",
    "BATCH_SIZE = 100\n",
    "NUM_CHECKPOINTS = 15\n",
    "\n",
    "# Calcular o total de exemplos de treinamento para atingir o STOP_POINT\n",
    "# Ajusta dinamicamente o número de exemplos de treinamento com base em STOP_POINT\n",
    "TOTAL_TRAINING_EXAMPLES = int(STOP_POINT * NUM_TRAINING_EXAMPLES)\n",
    "\n",
    "# Calcula o número de passos por época\n",
    "steps_per_epoch = TOTAL_TRAINING_EXAMPLES // (BATCH_SIZE * NUM_CHECKPOINTS)\n",
    "\n",
    "\n",
    "checkpoint_path = './checkpoints/model_checkpoint'\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_freq=steps_per_epoch)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "eval_dataset = eval_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=NUM_CHECKPOINTS,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_data=eval_dataset,\n",
    "                    callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "test_loss, test_acc = model.evaluate(eval_dataset, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
